{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure reproducibility\n",
    "Use a fixed seed such that all steps and results can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed handpicked to ensure all of the cleaning/pre-processing steps were visually shown\n",
    "SEED = 544\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Recurrent NN on Google's Word2Vec Model\n",
    "In this section, we will train an Recurrent Neural Network for sentiment analysis classification for both the binary and ternary cases.\n",
    "\n",
    "For part (a), use RNN cell with hidden state size of 50. Limit review length to 50, truncating longer ones and padding shorter reviews with zeros. For part (b), use a gated RNN cell instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "Load the pandas dataset from Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from disk\n",
    "data = pd.read_pickle('dataset.pkl')\n",
    "\n",
    "# Because this homework takes ages to run, I was forced to reduce the dataset's size by 60%\n",
    "data = data.drop(data[data['label'] == 0].sample(frac=.6).index)\n",
    "data = data.drop(data[data['label'] == 1].sample(frac=.6).index)\n",
    "data = data.drop(data[data['label'] == 2].sample(frac=.6).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Google Word2Vec model\n",
    "Load the w2v model from Q2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v_google = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the vectors for the reviews\n",
    "The input feature is the first 50 vectors of the review. Word with no encoding vectors are ignored. If a review has less than 50 vectors, the rest are filled with 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the given review body text into the first50 Word2Vec vector using a given trained word2vec model\n",
    "def create_first50_input_feature(text, wv):\n",
    "    vectors = []\n",
    "    # Will skip words that have no vectors\n",
    "    for word in str(text).split():\n",
    "        if word in wv:\n",
    "            vec = np.array(wv[word], np.float32)\n",
    "            vectors.append(vec)\n",
    "            # If we have our first 50 vectors, we can exit the loop\n",
    "            if len(vectors) == 50:\n",
    "                break\n",
    "    # The review does not have enough vectors, so we fill the rest with zeros\n",
    "    while len(vectors) < 50:\n",
    "        vectors.append(np.zeros((300,), dtype=np.float32))\n",
    "    return vectors\n",
    "\n",
    "# Create new column for the review's input feature for googles w2v\n",
    "data['google_input_features'] = data['cleaned_reviews'].apply(\n",
    "    lambda text: create_first50_input_feature(text, w2v_google)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functionality\n",
    "Below are functions and classes that group up implementations for code reuse and understanding. It is used for Q5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Returns training and testing data loaders\n",
    "def prepare_data(X_train, X_test, y_train, y_test, batch_size):\n",
    "    train_data = torch.tensor(X_train, device=device)\n",
    "    train_label = torch.tensor(y_train.values, dtype=torch.long, device=device)\n",
    "    train_tensor = TensorDataset(train_data, train_label)\n",
    "    train_loader = DataLoader(dataset=train_tensor, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    test_data = torch.tensor(X_test, device=device)\n",
    "    test_label = torch.tensor(y_test.values, dtype=torch.long, device=device)\n",
    "    test_tensor = TensorDataset(test_data, test_label)\n",
    "    test_loader = DataLoader(dataset=test_tensor, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Trains the model given the data_loader with max_epochs\n",
    "def train_model(model, max_epochs, data_loader):\n",
    "    model.train()\n",
    "    for epoch in range(max_epochs):\n",
    "        for idx, (X, y) in enumerate(data_loader):\n",
    "            # Match the shape needed for GRU which is (50, 300)\n",
    "            X = X.reshape(-1, model.sequence_length, model.input_size).to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            y_pred = model(X)\n",
    "            loss = model.criterion(y_pred, y)\n",
    "            model.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model.optimizer.step()\n",
    "            if (idx+1)%100 == 0:\n",
    "                print (f'Epoch [{epoch+1}/{max_epochs}], Step [{idx+1}], Loss: {loss.item():.3f}')\n",
    "\n",
    "# Returns the model's accuracy (0-1) given the data_loader\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X = X.reshape(-1, model.sequence_length, model.input_size).to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            scores = model(X)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "    return float(num_correct) / float(num_samples)\n",
    "\n",
    "# Reports the accuracy of the model\n",
    "def report_accuracy(model, text, data_loader):\n",
    "    accuracy = evaluate_model(model, data_loader)\n",
    "    print(f'{text}: accuracy is {accuracy:.3f}.')\n",
    "    print()\n",
    "\n",
    "# A Recurrent Neural Network with 1 hidden layer that uses CrossEntropyLoss and Adam optimizer\n",
    "class RNN(torch.nn.Module):\n",
    "    # Initializes the model with a single RNN layer using CrossEntropyLoss and Adam optimizer\n",
    "    def __init__(self, input_size, hidden_size, num_classes, learning_rate):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = 50\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, 1, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, num_classes)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Performs a forward pass\n",
    "    def forward(self, x):\n",
    "        # Initial the hidden state to zeros\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # Get the last output and discard all intermediate outputs\n",
    "        out = out[:,-1,:]\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "        \n",
    "# A Gated Recurrent Neural Network with 1 hidden layer that uses CrossEntropyLoss and Adam optimizer\n",
    "class GRU(torch.nn.Module):\n",
    "    # Initializes the model with a single GRU layer using CrossEntropyLoss and Adam optimizer\n",
    "    def __init__(self, input_size, hidden_size, num_classes, learning_rate):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = 50\n",
    "        self.gru = torch.nn.GRU(input_size, hidden_size, 1, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, num_classes)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Performs a forward pass\n",
    "    def forward(self, x):\n",
    "        # Initial the hidden state to zeros\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        # Get the last output and discard all intermediate outputs\n",
    "        out = out[:,-1,:]\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RNN and GRU on Google Word2Vec\n",
    "In this section, we will use an (a)RNN and (b)GRU as the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing data split (Binary)\n",
    "Split the data into two distinct parts (80% training, 20% testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "binary_data = data[data['label'] <= 1] # Only select class 0 (positive) and class 1 (negative)\n",
    "google_input_features = binary_data['google_input_features']\n",
    "binary_labels = binary_data['label']\n",
    "\n",
    "# Perform an 80-20 split for training and testing data on the binary data only\n",
    "X_train_google, X_test_google, y_train_google, y_test_google = train_test_split(\n",
    "    google_input_features,\n",
    "    binary_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Reshape from (num_samples,) to (num_samples, 50, 300)\n",
    "X_train_google = np.dstack(X_train_google) # (num_samples,) -> (50, 300, num_samples)\n",
    "X_train_google = np.moveaxis(X_train_google, -1, 0) # (50, 300, num_samples) -> (num_samples, 50, 300)\n",
    "X_test_google = np.dstack(X_test_google) # (num_samples,) -> (50, 300, num_samples)\n",
    "X_test_google = np.moveaxis(X_test_google, -1, 0) # (50, 300, num_samples) -> (num_samples, 50, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a.1 RNN with Google Word2Vec (Binary Case)\n",
    "Train an RNN for binary classification using Google Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100], Loss: 0.682\n",
      "Epoch [1/5], Step [200], Loss: 0.690\n",
      "Epoch [1/5], Step [300], Loss: 0.698\n",
      "Epoch [1/5], Step [400], Loss: 0.684\n",
      "Epoch [1/5], Step [500], Loss: 0.700\n",
      "Epoch [1/5], Step [600], Loss: 0.690\n",
      "Epoch [1/5], Step [700], Loss: 0.700\n",
      "Epoch [1/5], Step [800], Loss: 0.678\n",
      "Epoch [1/5], Step [900], Loss: 0.687\n",
      "Epoch [1/5], Step [1000], Loss: 0.586\n",
      "Epoch [1/5], Step [1100], Loss: 0.637\n",
      "Epoch [1/5], Step [1200], Loss: 0.650\n",
      "Epoch [1/5], Step [1300], Loss: 0.600\n",
      "Epoch [1/5], Step [1400], Loss: 0.553\n",
      "Epoch [1/5], Step [1500], Loss: 0.550\n",
      "Epoch [1/5], Step [1600], Loss: 0.496\n",
      "Epoch [1/5], Step [1700], Loss: 0.647\n",
      "Epoch [1/5], Step [1800], Loss: 0.448\n",
      "Epoch [1/5], Step [1900], Loss: 0.439\n",
      "Epoch [2/5], Step [100], Loss: 0.659\n",
      "Epoch [2/5], Step [200], Loss: 0.578\n",
      "Epoch [2/5], Step [300], Loss: 0.720\n",
      "Epoch [2/5], Step [400], Loss: 0.558\n",
      "Epoch [2/5], Step [500], Loss: 0.427\n",
      "Epoch [2/5], Step [600], Loss: 0.620\n",
      "Epoch [2/5], Step [700], Loss: 0.404\n",
      "Epoch [2/5], Step [800], Loss: 0.497\n",
      "Epoch [2/5], Step [900], Loss: 0.507\n",
      "Epoch [2/5], Step [1000], Loss: 0.580\n",
      "Epoch [2/5], Step [1100], Loss: 0.492\n",
      "Epoch [2/5], Step [1200], Loss: 0.620\n",
      "Epoch [2/5], Step [1300], Loss: 0.785\n",
      "Epoch [2/5], Step [1400], Loss: 0.577\n",
      "Epoch [2/5], Step [1500], Loss: 0.460\n",
      "Epoch [2/5], Step [1600], Loss: 0.377\n",
      "Epoch [2/5], Step [1700], Loss: 0.443\n",
      "Epoch [2/5], Step [1800], Loss: 0.475\n",
      "Epoch [2/5], Step [1900], Loss: 0.593\n",
      "Epoch [3/5], Step [100], Loss: 0.330\n",
      "Epoch [3/5], Step [200], Loss: 0.526\n",
      "Epoch [3/5], Step [300], Loss: 0.308\n",
      "Epoch [3/5], Step [400], Loss: 0.559\n",
      "Epoch [3/5], Step [500], Loss: 0.552\n",
      "Epoch [3/5], Step [600], Loss: 0.762\n",
      "Epoch [3/5], Step [700], Loss: 0.394\n",
      "Epoch [3/5], Step [800], Loss: 0.544\n",
      "Epoch [3/5], Step [900], Loss: 0.356\n",
      "Epoch [3/5], Step [1000], Loss: 0.596\n",
      "Epoch [3/5], Step [1100], Loss: 0.342\n",
      "Epoch [3/5], Step [1200], Loss: 0.631\n",
      "Epoch [3/5], Step [1300], Loss: 0.484\n",
      "Epoch [3/5], Step [1400], Loss: 0.461\n",
      "Epoch [3/5], Step [1500], Loss: 0.527\n",
      "Epoch [3/5], Step [1600], Loss: 0.545\n",
      "Epoch [3/5], Step [1700], Loss: 0.569\n",
      "Epoch [3/5], Step [1800], Loss: 0.365\n",
      "Epoch [3/5], Step [1900], Loss: 0.463\n",
      "Epoch [4/5], Step [100], Loss: 0.258\n",
      "Epoch [4/5], Step [200], Loss: 0.455\n",
      "Epoch [4/5], Step [300], Loss: 0.563\n",
      "Epoch [4/5], Step [400], Loss: 0.531\n",
      "Epoch [4/5], Step [500], Loss: 0.400\n",
      "Epoch [4/5], Step [600], Loss: 0.429\n",
      "Epoch [4/5], Step [700], Loss: 0.458\n",
      "Epoch [4/5], Step [800], Loss: 0.626\n",
      "Epoch [4/5], Step [900], Loss: 0.412\n",
      "Epoch [4/5], Step [1000], Loss: 0.669\n",
      "Epoch [4/5], Step [1100], Loss: 0.548\n",
      "Epoch [4/5], Step [1200], Loss: 0.303\n",
      "Epoch [4/5], Step [1300], Loss: 0.343\n",
      "Epoch [4/5], Step [1400], Loss: 0.439\n",
      "Epoch [4/5], Step [1500], Loss: 0.334\n",
      "Epoch [4/5], Step [1600], Loss: 0.537\n",
      "Epoch [4/5], Step [1700], Loss: 0.523\n",
      "Epoch [4/5], Step [1800], Loss: 0.345\n",
      "Epoch [4/5], Step [1900], Loss: 0.561\n",
      "Epoch [5/5], Step [100], Loss: 0.513\n",
      "Epoch [5/5], Step [200], Loss: 0.470\n",
      "Epoch [5/5], Step [300], Loss: 0.314\n",
      "Epoch [5/5], Step [400], Loss: 0.477\n",
      "Epoch [5/5], Step [500], Loss: 0.457\n",
      "Epoch [5/5], Step [600], Loss: 0.286\n",
      "Epoch [5/5], Step [700], Loss: 0.336\n",
      "Epoch [5/5], Step [800], Loss: 0.332\n",
      "Epoch [5/5], Step [900], Loss: 0.560\n",
      "Epoch [5/5], Step [1000], Loss: 0.465\n",
      "Epoch [5/5], Step [1100], Loss: 0.445\n",
      "Epoch [5/5], Step [1200], Loss: 0.427\n",
      "Epoch [5/5], Step [1300], Loss: 0.364\n",
      "Epoch [5/5], Step [1400], Loss: 0.482\n",
      "Epoch [5/5], Step [1500], Loss: 0.543\n",
      "Epoch [5/5], Step [1600], Loss: 0.450\n",
      "Epoch [5/5], Step [1700], Loss: 0.516\n",
      "Epoch [5/5], Step [1800], Loss: 0.464\n",
      "Epoch [5/5], Step [1900], Loss: 0.496\n",
      "Google W2V - RNN - Binary: accuracy is 0.807.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Binary RNN - Google Word2Vec\n",
    "\n",
    "# Prepare the data for PyTorch\n",
    "train_loader, test_loader = prepare_data(X_train_google, X_test_google, y_train_google, y_test_google, batch_size=32)\n",
    "\n",
    "# Create RNN for binary classification\n",
    "model = RNN(input_size=300, hidden_size=50, num_classes=2, learning_rate=1e-4)\n",
    "\n",
    "# Train the RNN\n",
    "train_model(model, max_epochs=5, data_loader=train_loader)\n",
    "\n",
    "# Evaluate RNN\n",
    "report_accuracy(model, 'Google W2V - RNN - Binary', data_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b.1 GRU with Google Word2Vec (Binary Case)\n",
    "Train an GRU for binary classification using Google Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100], Loss: 0.676\n",
      "Epoch [1/5], Step [200], Loss: 0.691\n",
      "Epoch [1/5], Step [300], Loss: 0.706\n",
      "Epoch [1/5], Step [400], Loss: 0.701\n",
      "Epoch [1/5], Step [500], Loss: 0.706\n",
      "Epoch [1/5], Step [600], Loss: 0.683\n",
      "Epoch [1/5], Step [700], Loss: 0.695\n",
      "Epoch [1/5], Step [800], Loss: 0.696\n",
      "Epoch [1/5], Step [900], Loss: 0.710\n",
      "Epoch [1/5], Step [1000], Loss: 0.700\n",
      "Epoch [1/5], Step [1100], Loss: 0.681\n",
      "Epoch [1/5], Step [1200], Loss: 0.685\n",
      "Epoch [1/5], Step [1300], Loss: 0.641\n",
      "Epoch [1/5], Step [1400], Loss: 0.627\n",
      "Epoch [1/5], Step [1500], Loss: 0.655\n",
      "Epoch [1/5], Step [1600], Loss: 0.593\n",
      "Epoch [1/5], Step [1700], Loss: 0.342\n",
      "Epoch [1/5], Step [1800], Loss: 0.463\n",
      "Epoch [1/5], Step [1900], Loss: 0.435\n",
      "Epoch [2/5], Step [100], Loss: 0.599\n",
      "Epoch [2/5], Step [200], Loss: 0.344\n",
      "Epoch [2/5], Step [300], Loss: 0.434\n",
      "Epoch [2/5], Step [400], Loss: 0.460\n",
      "Epoch [2/5], Step [500], Loss: 0.325\n",
      "Epoch [2/5], Step [600], Loss: 0.379\n",
      "Epoch [2/5], Step [700], Loss: 0.305\n",
      "Epoch [2/5], Step [800], Loss: 0.437\n",
      "Epoch [2/5], Step [900], Loss: 0.501\n",
      "Epoch [2/5], Step [1000], Loss: 0.404\n",
      "Epoch [2/5], Step [1100], Loss: 0.434\n",
      "Epoch [2/5], Step [1200], Loss: 0.396\n",
      "Epoch [2/5], Step [1300], Loss: 0.243\n",
      "Epoch [2/5], Step [1400], Loss: 0.347\n",
      "Epoch [2/5], Step [1500], Loss: 0.638\n",
      "Epoch [2/5], Step [1600], Loss: 0.500\n",
      "Epoch [2/5], Step [1700], Loss: 0.453\n",
      "Epoch [2/5], Step [1800], Loss: 0.325\n",
      "Epoch [2/5], Step [1900], Loss: 0.445\n",
      "Epoch [3/5], Step [100], Loss: 0.250\n",
      "Epoch [3/5], Step [200], Loss: 0.384\n",
      "Epoch [3/5], Step [300], Loss: 0.358\n",
      "Epoch [3/5], Step [400], Loss: 0.344\n",
      "Epoch [3/5], Step [500], Loss: 0.332\n",
      "Epoch [3/5], Step [600], Loss: 0.331\n",
      "Epoch [3/5], Step [700], Loss: 0.428\n",
      "Epoch [3/5], Step [800], Loss: 0.543\n",
      "Epoch [3/5], Step [900], Loss: 0.418\n",
      "Epoch [3/5], Step [1000], Loss: 0.234\n",
      "Epoch [3/5], Step [1100], Loss: 0.469\n",
      "Epoch [3/5], Step [1200], Loss: 0.534\n",
      "Epoch [3/5], Step [1300], Loss: 0.349\n",
      "Epoch [3/5], Step [1400], Loss: 0.440\n",
      "Epoch [3/5], Step [1500], Loss: 0.419\n",
      "Epoch [3/5], Step [1600], Loss: 0.466\n",
      "Epoch [3/5], Step [1700], Loss: 0.237\n",
      "Epoch [3/5], Step [1800], Loss: 0.322\n",
      "Epoch [3/5], Step [1900], Loss: 0.471\n",
      "Epoch [4/5], Step [100], Loss: 0.304\n",
      "Epoch [4/5], Step [200], Loss: 0.389\n",
      "Epoch [4/5], Step [300], Loss: 0.427\n",
      "Epoch [4/5], Step [400], Loss: 0.346\n",
      "Epoch [4/5], Step [500], Loss: 0.395\n",
      "Epoch [4/5], Step [600], Loss: 0.370\n",
      "Epoch [4/5], Step [700], Loss: 0.198\n",
      "Epoch [4/5], Step [800], Loss: 0.406\n",
      "Epoch [4/5], Step [900], Loss: 0.513\n",
      "Epoch [4/5], Step [1000], Loss: 0.234\n",
      "Epoch [4/5], Step [1100], Loss: 0.379\n",
      "Epoch [4/5], Step [1200], Loss: 0.371\n",
      "Epoch [4/5], Step [1300], Loss: 0.298\n",
      "Epoch [4/5], Step [1400], Loss: 0.381\n",
      "Epoch [4/5], Step [1500], Loss: 0.298\n",
      "Epoch [4/5], Step [1600], Loss: 0.316\n",
      "Epoch [4/5], Step [1700], Loss: 0.356\n",
      "Epoch [4/5], Step [1800], Loss: 0.282\n",
      "Epoch [4/5], Step [1900], Loss: 0.376\n",
      "Epoch [5/5], Step [100], Loss: 0.574\n",
      "Epoch [5/5], Step [200], Loss: 0.456\n",
      "Epoch [5/5], Step [300], Loss: 0.314\n",
      "Epoch [5/5], Step [400], Loss: 0.433\n",
      "Epoch [5/5], Step [500], Loss: 0.396\n",
      "Epoch [5/5], Step [600], Loss: 0.263\n",
      "Epoch [5/5], Step [700], Loss: 0.382\n",
      "Epoch [5/5], Step [800], Loss: 0.451\n",
      "Epoch [5/5], Step [900], Loss: 0.304\n",
      "Epoch [5/5], Step [1000], Loss: 0.406\n",
      "Epoch [5/5], Step [1100], Loss: 0.311\n",
      "Epoch [5/5], Step [1200], Loss: 0.141\n",
      "Epoch [5/5], Step [1300], Loss: 0.309\n",
      "Epoch [5/5], Step [1400], Loss: 0.503\n",
      "Epoch [5/5], Step [1500], Loss: 0.302\n",
      "Epoch [5/5], Step [1600], Loss: 0.364\n",
      "Epoch [5/5], Step [1700], Loss: 0.582\n",
      "Epoch [5/5], Step [1800], Loss: 0.321\n",
      "Epoch [5/5], Step [1900], Loss: 0.468\n",
      "Google W2V - GRU - Binary: accuracy is 0.841.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Binary GRU - Google Word2Vec\n",
    "\n",
    "# Prepare the data for PyTorch\n",
    "train_loader, test_loader = prepare_data(X_train_google, X_test_google, y_train_google, y_test_google, batch_size=32)\n",
    "\n",
    "# Create GRU for binary classification\n",
    "model = GRU(input_size=300, hidden_size=50, num_classes=2, learning_rate=1e-4)\n",
    "\n",
    "# Train the GRU\n",
    "train_model(model, max_epochs=5, data_loader=train_loader)\n",
    "\n",
    "# Evaluate RNN\n",
    "report_accuracy(model, 'Google W2V - GRU - Binary', data_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing data split (Ternary)\n",
    "Split the data into two distinct parts (80% training, 20% testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_input_features = data['google_input_features']\n",
    "ternary_labels = data['label']\n",
    "\n",
    "# Perform an 80-20 split for training and testing data on the complete dataset\n",
    "X_train_google, X_test_google, y_train_google, y_test_google = train_test_split(\n",
    "    google_input_features,\n",
    "    ternary_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Reshape from (num_samples,) to (num_samples, 50, 300)\n",
    "X_train_google = np.dstack(X_train_google) # (num_samples,) -> (50, 300, num_samples)\n",
    "X_train_google = np.moveaxis(X_train_google, -1, 0) # (50, 300, num_samples) -> (num_samples, 50, 300)\n",
    "X_test_google = np.dstack(X_test_google) # (num_samples,) -> (50, 300, num_samples)\n",
    "X_test_google = np.moveaxis(X_test_google, -1, 0) # (50, 300, num_samples) -> (num_samples, 50, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a.2 RNN with Google Word2Vec (Ternary Case)\n",
    "Train an RNN for ternary classification using Google Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100], Loss: 1.035\n",
      "Epoch [1/5], Step [200], Loss: 1.039\n",
      "Epoch [1/5], Step [300], Loss: 1.100\n",
      "Epoch [1/5], Step [400], Loss: 1.107\n",
      "Epoch [1/5], Step [500], Loss: 1.017\n",
      "Epoch [1/5], Step [600], Loss: 1.022\n",
      "Epoch [1/5], Step [700], Loss: 1.060\n",
      "Epoch [1/5], Step [800], Loss: 0.999\n",
      "Epoch [1/5], Step [900], Loss: 1.008\n",
      "Epoch [1/5], Step [1000], Loss: 1.060\n",
      "Epoch [1/5], Step [1100], Loss: 1.065\n",
      "Epoch [1/5], Step [1200], Loss: 1.032\n",
      "Epoch [1/5], Step [1300], Loss: 1.060\n",
      "Epoch [1/5], Step [1400], Loss: 1.032\n",
      "Epoch [1/5], Step [1500], Loss: 1.069\n",
      "Epoch [1/5], Step [1600], Loss: 1.063\n",
      "Epoch [1/5], Step [1700], Loss: 1.014\n",
      "Epoch [1/5], Step [1800], Loss: 1.075\n",
      "Epoch [1/5], Step [1900], Loss: 1.143\n",
      "Epoch [1/5], Step [2000], Loss: 1.019\n",
      "Epoch [1/5], Step [2100], Loss: 1.017\n",
      "Epoch [1/5], Step [2200], Loss: 1.096\n",
      "Epoch [1/5], Step [2300], Loss: 0.995\n",
      "Epoch [1/5], Step [2400], Loss: 1.110\n",
      "Epoch [2/5], Step [100], Loss: 1.079\n",
      "Epoch [2/5], Step [200], Loss: 0.962\n",
      "Epoch [2/5], Step [300], Loss: 1.031\n",
      "Epoch [2/5], Step [400], Loss: 0.873\n",
      "Epoch [2/5], Step [500], Loss: 0.922\n",
      "Epoch [2/5], Step [600], Loss: 0.954\n",
      "Epoch [2/5], Step [700], Loss: 0.969\n",
      "Epoch [2/5], Step [800], Loss: 1.090\n",
      "Epoch [2/5], Step [900], Loss: 1.028\n",
      "Epoch [2/5], Step [1000], Loss: 1.021\n",
      "Epoch [2/5], Step [1100], Loss: 0.908\n",
      "Epoch [2/5], Step [1200], Loss: 0.988\n",
      "Epoch [2/5], Step [1300], Loss: 0.899\n",
      "Epoch [2/5], Step [1400], Loss: 0.985\n",
      "Epoch [2/5], Step [1500], Loss: 0.820\n",
      "Epoch [2/5], Step [1600], Loss: 0.902\n",
      "Epoch [2/5], Step [1700], Loss: 1.051\n",
      "Epoch [2/5], Step [1800], Loss: 1.022\n",
      "Epoch [2/5], Step [1900], Loss: 1.027\n",
      "Epoch [2/5], Step [2000], Loss: 0.795\n",
      "Epoch [2/5], Step [2100], Loss: 0.885\n",
      "Epoch [2/5], Step [2200], Loss: 0.828\n",
      "Epoch [2/5], Step [2300], Loss: 0.856\n",
      "Epoch [2/5], Step [2400], Loss: 1.044\n",
      "Epoch [3/5], Step [100], Loss: 1.019\n",
      "Epoch [3/5], Step [200], Loss: 0.857\n",
      "Epoch [3/5], Step [300], Loss: 1.130\n",
      "Epoch [3/5], Step [400], Loss: 0.834\n",
      "Epoch [3/5], Step [500], Loss: 0.963\n",
      "Epoch [3/5], Step [600], Loss: 1.001\n",
      "Epoch [3/5], Step [700], Loss: 0.923\n",
      "Epoch [3/5], Step [800], Loss: 0.987\n",
      "Epoch [3/5], Step [900], Loss: 0.935\n",
      "Epoch [3/5], Step [1000], Loss: 0.883\n",
      "Epoch [3/5], Step [1100], Loss: 0.910\n",
      "Epoch [3/5], Step [1200], Loss: 0.823\n",
      "Epoch [3/5], Step [1300], Loss: 0.749\n",
      "Epoch [3/5], Step [1400], Loss: 0.811\n",
      "Epoch [3/5], Step [1500], Loss: 0.857\n",
      "Epoch [3/5], Step [1600], Loss: 0.926\n",
      "Epoch [3/5], Step [1700], Loss: 0.949\n",
      "Epoch [3/5], Step [1800], Loss: 1.051\n",
      "Epoch [3/5], Step [1900], Loss: 0.947\n",
      "Epoch [3/5], Step [2000], Loss: 0.791\n",
      "Epoch [3/5], Step [2100], Loss: 0.989\n",
      "Epoch [3/5], Step [2200], Loss: 0.826\n",
      "Epoch [3/5], Step [2300], Loss: 0.833\n",
      "Epoch [3/5], Step [2400], Loss: 1.151\n",
      "Epoch [4/5], Step [100], Loss: 0.777\n",
      "Epoch [4/5], Step [200], Loss: 0.942\n",
      "Epoch [4/5], Step [300], Loss: 0.815\n",
      "Epoch [4/5], Step [400], Loss: 0.801\n",
      "Epoch [4/5], Step [500], Loss: 0.717\n",
      "Epoch [4/5], Step [600], Loss: 0.827\n",
      "Epoch [4/5], Step [700], Loss: 0.839\n",
      "Epoch [4/5], Step [800], Loss: 0.862\n",
      "Epoch [4/5], Step [900], Loss: 0.900\n",
      "Epoch [4/5], Step [1000], Loss: 0.980\n",
      "Epoch [4/5], Step [1100], Loss: 0.987\n",
      "Epoch [4/5], Step [1200], Loss: 0.956\n",
      "Epoch [4/5], Step [1300], Loss: 0.779\n",
      "Epoch [4/5], Step [1400], Loss: 0.869\n",
      "Epoch [4/5], Step [1500], Loss: 0.780\n",
      "Epoch [4/5], Step [1600], Loss: 0.743\n",
      "Epoch [4/5], Step [1700], Loss: 0.781\n",
      "Epoch [4/5], Step [1800], Loss: 0.777\n",
      "Epoch [4/5], Step [1900], Loss: 0.939\n",
      "Epoch [4/5], Step [2000], Loss: 0.856\n",
      "Epoch [4/5], Step [2100], Loss: 0.972\n",
      "Epoch [4/5], Step [2200], Loss: 0.841\n",
      "Epoch [4/5], Step [2300], Loss: 0.754\n",
      "Epoch [4/5], Step [2400], Loss: 0.895\n",
      "Epoch [5/5], Step [100], Loss: 1.022\n",
      "Epoch [5/5], Step [200], Loss: 0.756\n",
      "Epoch [5/5], Step [300], Loss: 0.761\n",
      "Epoch [5/5], Step [400], Loss: 0.955\n",
      "Epoch [5/5], Step [500], Loss: 0.746\n",
      "Epoch [5/5], Step [600], Loss: 0.725\n",
      "Epoch [5/5], Step [700], Loss: 0.735\n",
      "Epoch [5/5], Step [800], Loss: 1.076\n",
      "Epoch [5/5], Step [900], Loss: 0.967\n",
      "Epoch [5/5], Step [1000], Loss: 0.891\n",
      "Epoch [5/5], Step [1100], Loss: 0.855\n",
      "Epoch [5/5], Step [1200], Loss: 0.784\n",
      "Epoch [5/5], Step [1300], Loss: 0.841\n",
      "Epoch [5/5], Step [1400], Loss: 1.050\n",
      "Epoch [5/5], Step [1500], Loss: 0.746\n",
      "Epoch [5/5], Step [1600], Loss: 0.718\n",
      "Epoch [5/5], Step [1700], Loss: 1.051\n",
      "Epoch [5/5], Step [1800], Loss: 0.747\n",
      "Epoch [5/5], Step [1900], Loss: 0.844\n",
      "Epoch [5/5], Step [2000], Loss: 0.745\n",
      "Epoch [5/5], Step [2100], Loss: 1.006\n",
      "Epoch [5/5], Step [2200], Loss: 0.878\n",
      "Epoch [5/5], Step [2300], Loss: 0.963\n",
      "Epoch [5/5], Step [2400], Loss: 0.975\n",
      "Google W2V - RNN - Ternary: accuracy is 0.646.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ternary RNN - Google Word2Vec\n",
    "\n",
    "# Prepare the data for PyTorch\n",
    "train_loader, test_loader = prepare_data(X_train_google, X_test_google, y_train_google, y_test_google, batch_size=32)\n",
    "\n",
    "# Create RNN for ternary classification\n",
    "model = RNN(input_size=300, hidden_size=50, num_classes=3, learning_rate=1e-4)\n",
    "\n",
    "# Train the RNN\n",
    "train_model(model, max_epochs=5, data_loader=train_loader)\n",
    "\n",
    "# Evaluate RNN\n",
    "report_accuracy(model, 'Google W2V - RNN - Ternary', data_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b.2 GRU with Google Word2Vec (Ternary Case)\n",
    "Train an GRU for ternary classification using Google Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100], Loss: 1.062\n",
      "Epoch [1/5], Step [200], Loss: 1.020\n",
      "Epoch [1/5], Step [300], Loss: 1.116\n",
      "Epoch [1/5], Step [400], Loss: 0.991\n",
      "Epoch [1/5], Step [500], Loss: 1.083\n",
      "Epoch [1/5], Step [600], Loss: 0.980\n",
      "Epoch [1/5], Step [700], Loss: 1.131\n",
      "Epoch [1/5], Step [800], Loss: 0.932\n",
      "Epoch [1/5], Step [900], Loss: 1.017\n",
      "Epoch [1/5], Step [1000], Loss: 1.014\n",
      "Epoch [1/5], Step [1100], Loss: 1.019\n",
      "Epoch [1/5], Step [1200], Loss: 1.076\n",
      "Epoch [1/5], Step [1300], Loss: 1.050\n",
      "Epoch [1/5], Step [1400], Loss: 1.001\n",
      "Epoch [1/5], Step [1500], Loss: 1.082\n",
      "Epoch [1/5], Step [1600], Loss: 1.070\n",
      "Epoch [1/5], Step [1700], Loss: 1.132\n",
      "Epoch [1/5], Step [1800], Loss: 1.014\n",
      "Epoch [1/5], Step [1900], Loss: 1.071\n",
      "Epoch [1/5], Step [2000], Loss: 0.988\n",
      "Epoch [1/5], Step [2100], Loss: 0.960\n",
      "Epoch [1/5], Step [2200], Loss: 1.025\n",
      "Epoch [1/5], Step [2300], Loss: 0.927\n",
      "Epoch [1/5], Step [2400], Loss: 0.831\n",
      "Epoch [2/5], Step [100], Loss: 0.673\n",
      "Epoch [2/5], Step [200], Loss: 0.697\n",
      "Epoch [2/5], Step [300], Loss: 0.702\n",
      "Epoch [2/5], Step [400], Loss: 0.959\n",
      "Epoch [2/5], Step [500], Loss: 1.093\n",
      "Epoch [2/5], Step [600], Loss: 0.845\n",
      "Epoch [2/5], Step [700], Loss: 0.833\n",
      "Epoch [2/5], Step [800], Loss: 0.889\n",
      "Epoch [2/5], Step [900], Loss: 0.764\n",
      "Epoch [2/5], Step [1000], Loss: 1.036\n",
      "Epoch [2/5], Step [1100], Loss: 0.910\n",
      "Epoch [2/5], Step [1200], Loss: 0.779\n",
      "Epoch [2/5], Step [1300], Loss: 0.978\n",
      "Epoch [2/5], Step [1400], Loss: 0.764\n",
      "Epoch [2/5], Step [1500], Loss: 0.852\n",
      "Epoch [2/5], Step [1600], Loss: 0.872\n",
      "Epoch [2/5], Step [1700], Loss: 0.661\n",
      "Epoch [2/5], Step [1800], Loss: 0.871\n",
      "Epoch [2/5], Step [1900], Loss: 0.698\n",
      "Epoch [2/5], Step [2000], Loss: 0.866\n",
      "Epoch [2/5], Step [2100], Loss: 0.636\n",
      "Epoch [2/5], Step [2200], Loss: 0.667\n",
      "Epoch [2/5], Step [2300], Loss: 0.738\n",
      "Epoch [2/5], Step [2400], Loss: 0.782\n",
      "Epoch [3/5], Step [100], Loss: 0.896\n",
      "Epoch [3/5], Step [200], Loss: 0.489\n",
      "Epoch [3/5], Step [300], Loss: 0.770\n",
      "Epoch [3/5], Step [400], Loss: 0.690\n",
      "Epoch [3/5], Step [500], Loss: 0.660\n",
      "Epoch [3/5], Step [600], Loss: 0.925\n",
      "Epoch [3/5], Step [700], Loss: 0.723\n",
      "Epoch [3/5], Step [800], Loss: 0.838\n",
      "Epoch [3/5], Step [900], Loss: 0.722\n",
      "Epoch [3/5], Step [1000], Loss: 0.751\n",
      "Epoch [3/5], Step [1100], Loss: 0.904\n",
      "Epoch [3/5], Step [1200], Loss: 0.846\n",
      "Epoch [3/5], Step [1300], Loss: 0.661\n",
      "Epoch [3/5], Step [1400], Loss: 0.732\n",
      "Epoch [3/5], Step [1500], Loss: 0.655\n",
      "Epoch [3/5], Step [1600], Loss: 0.504\n",
      "Epoch [3/5], Step [1700], Loss: 0.819\n",
      "Epoch [3/5], Step [1800], Loss: 0.836\n",
      "Epoch [3/5], Step [1900], Loss: 0.846\n",
      "Epoch [3/5], Step [2000], Loss: 0.703\n",
      "Epoch [3/5], Step [2100], Loss: 0.825\n",
      "Epoch [3/5], Step [2200], Loss: 0.927\n",
      "Epoch [3/5], Step [2300], Loss: 0.694\n",
      "Epoch [3/5], Step [2400], Loss: 0.866\n",
      "Epoch [4/5], Step [100], Loss: 0.814\n",
      "Epoch [4/5], Step [200], Loss: 0.730\n",
      "Epoch [4/5], Step [300], Loss: 0.718\n",
      "Epoch [4/5], Step [400], Loss: 0.629\n",
      "Epoch [4/5], Step [500], Loss: 0.700\n",
      "Epoch [4/5], Step [600], Loss: 0.897\n",
      "Epoch [4/5], Step [700], Loss: 1.149\n",
      "Epoch [4/5], Step [800], Loss: 0.814\n",
      "Epoch [4/5], Step [900], Loss: 0.735\n",
      "Epoch [4/5], Step [1000], Loss: 0.687\n",
      "Epoch [4/5], Step [1100], Loss: 0.897\n",
      "Epoch [4/5], Step [1200], Loss: 0.574\n",
      "Epoch [4/5], Step [1300], Loss: 0.736\n",
      "Epoch [4/5], Step [1400], Loss: 0.671\n",
      "Epoch [4/5], Step [1500], Loss: 0.767\n",
      "Epoch [4/5], Step [1600], Loss: 0.720\n",
      "Epoch [4/5], Step [1700], Loss: 0.734\n",
      "Epoch [4/5], Step [1800], Loss: 0.707\n",
      "Epoch [4/5], Step [1900], Loss: 0.895\n",
      "Epoch [4/5], Step [2000], Loss: 0.865\n",
      "Epoch [4/5], Step [2100], Loss: 0.548\n",
      "Epoch [4/5], Step [2200], Loss: 0.786\n",
      "Epoch [4/5], Step [2300], Loss: 0.857\n",
      "Epoch [4/5], Step [2400], Loss: 0.766\n",
      "Epoch [5/5], Step [100], Loss: 0.609\n",
      "Epoch [5/5], Step [200], Loss: 0.555\n",
      "Epoch [5/5], Step [300], Loss: 0.587\n",
      "Epoch [5/5], Step [400], Loss: 0.605\n",
      "Epoch [5/5], Step [500], Loss: 0.764\n",
      "Epoch [5/5], Step [600], Loss: 0.732\n",
      "Epoch [5/5], Step [700], Loss: 0.839\n",
      "Epoch [5/5], Step [800], Loss: 0.707\n",
      "Epoch [5/5], Step [900], Loss: 0.950\n",
      "Epoch [5/5], Step [1000], Loss: 0.754\n",
      "Epoch [5/5], Step [1100], Loss: 0.940\n",
      "Epoch [5/5], Step [1200], Loss: 0.884\n",
      "Epoch [5/5], Step [1300], Loss: 0.861\n",
      "Epoch [5/5], Step [1400], Loss: 0.868\n",
      "Epoch [5/5], Step [1500], Loss: 0.764\n",
      "Epoch [5/5], Step [1600], Loss: 0.720\n",
      "Epoch [5/5], Step [1700], Loss: 0.734\n",
      "Epoch [5/5], Step [1800], Loss: 0.853\n",
      "Epoch [5/5], Step [1900], Loss: 0.826\n",
      "Epoch [5/5], Step [2000], Loss: 0.722\n",
      "Epoch [5/5], Step [2100], Loss: 0.735\n",
      "Epoch [5/5], Step [2200], Loss: 0.784\n",
      "Epoch [5/5], Step [2300], Loss: 0.676\n",
      "Epoch [5/5], Step [2400], Loss: 0.688\n",
      "Google W2V - GRU - Ternary: accuracy is 0.676.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ternary GRU - Google Word2Vec\n",
    "\n",
    "# Prepare the data for PyTorch\n",
    "train_loader, test_loader = prepare_data(X_train_google, X_test_google, y_train_google, y_test_google, batch_size=32)\n",
    "\n",
    "# Create GRU for ternary classification\n",
    "model = GRU(input_size=300, hidden_size=50, num_classes=3, learning_rate=1e-4)\n",
    "\n",
    "# Train the GRU\n",
    "train_model(model, max_epochs=5, data_loader=train_loader)\n",
    "\n",
    "# Evaluate RNN\n",
    "report_accuracy(model, 'Google W2V - GRU - Ternary', data_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "At first glance, the accuracies don't seem that great. However, keep in mind, that only 40% of the dataset is being used for training because my PC cannot handle the full dataset. Moreover, the number of epochs was limited to 5 because it was already too time consuming (hours). Having said that, accuracies in the 80s (binary) and 60s (ternary) is decent and I strongly believe that given more RAM and computation power, the RNN and GRU models will reach the 90% accuracy territory because the loss is slowly decreasing throughout the 5 epochs as shown above. Moreover, GRU seems to outperform RNN in our experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
