{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure reproducibility\n",
    "Use a fixed seed such that all steps and results can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed handpicked to ensure all of the cleaning/pre-processing steps were visually shown\n",
    "SEED = 544\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feedforward NN\n",
    "In this section, we will train a Multilayer Perceptron for sentiment analysis classification for both the binary and ternary cases. Per the homework requirements, our network will consists of two hidden layers, each with 50 and 10 nodes, respectively. We will use cross entropy loss and ADAM for optimizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functionality\n",
    "Below are functions and classes that group up implementations for code reuse and understanding. It is used for Q4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Credit: From PyTorch's documentation\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Returns training and testing data loaders\n",
    "def prepare_data(X_train, X_test, y_train, y_test, batch_size):\n",
    "    train_data = torch.tensor(X_train, device=device)\n",
    "    train_label = torch.tensor(y_train.values, dtype=torch.long, device=device)\n",
    "    train_tensor = TensorDataset(train_data, train_label)\n",
    "    train_loader = DataLoader(dataset=train_tensor, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    test_data = torch.tensor(X_test, device=device)\n",
    "    test_label = torch.tensor(y_test.values, dtype=torch.long, device=device)\n",
    "    test_tensor = TensorDataset(test_data, test_label)\n",
    "    test_loader = DataLoader(dataset=test_tensor, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# A Multilayer Perceptron with 2 hidden layers that uses CrossEntropyLoss and Adam optimizer\n",
    "class MLP:\n",
    "    # Creates a model with Cross Entropy Loss and Adam optimizer\n",
    "    def __init__(self, num_input, num_classes, learning_rate):\n",
    "        self.model = MLP.create_model(num_input, 50, 10, num_classes)\n",
    "        self.model.apply(MLP.initialize_weights)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "    # Trains the model given the data_loader with max_epochs\n",
    "    def train(self, max_epochs, data_loader):\n",
    "        self.model.train()\n",
    "        for epoch in range(max_epochs):\n",
    "            for idx, (X, y) in enumerate(data_loader):\n",
    "                X = X.to(device=device)\n",
    "                y = y.to(device=device)\n",
    "                y_pred = self.model(X)\n",
    "                loss = self.criterion(y_pred, y)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "    # Returns the model's accuracy (0-1) given the data_loader\n",
    "    def evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in data_loader:\n",
    "                X = X.to(device=device)\n",
    "                y = y.to(device=device)\n",
    "                scores = self.model(X)\n",
    "                _, predictions = scores.max(1)\n",
    "                num_correct += (predictions == y).sum()\n",
    "                num_samples += predictions.size(0)\n",
    "        return float(num_correct) / float(num_samples)\n",
    "\n",
    "    # Reports the accuracy of the model\n",
    "    def report_accuracy(self, text, data_loader):\n",
    "        accuracy = self.evaluate(data_loader)\n",
    "        print(f'{text}: accuracy is {accuracy:.3f}.')\n",
    "        print()\n",
    "\n",
    "    # Creates a 2 hidden layer NN with the specified sizes, with ReLU activations\n",
    "    @staticmethod\n",
    "    def create_model(n_input, n_h1, n_h2, n_classes):\n",
    "        model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_input, n_h1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(n_h1, n_h2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(n_h2, n_classes)\n",
    "        )\n",
    "        model.to(device)\n",
    "        return model\n",
    "    \n",
    "    # Initializes linear layers to 0 with a bias of 1\n",
    "    @staticmethod\n",
    "    def initialize_weights(m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.zeros_(m.weight)\n",
    "            torch.nn.init.ones_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "Load the pandas dataset from Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from disk\n",
    "data = pd.read_pickle('dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load both Word2Vec models\n",
    "Load the w2v models from Q2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v_google = api.load('word2vec-google-news-300')\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "w2v_own = KeyedVectors.load('my_w2v.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. First-10 Input Features in Multi-Layer Perceptron\n",
    "In this section, we shall build an MLP that takes in the the first-10 vectors as its input feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b.1 Create the first-10 input features for the reviews\n",
    "Input feature is the first 10 Word2Vec vector concatenated for each review. Words with no encoding vectors are ignored. If a review has less than 10 vectors, the rest are filled with 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the given review body text into the first10 Word2Vec vector using a given trained word2vec model\n",
    "def create_first10_input_feature(text, wv):\n",
    "    vectors = []\n",
    "    # Will skip words that have no vectors\n",
    "    for word in str(text).split():\n",
    "        if word in wv:\n",
    "            vec = np.array(wv[word], dtype=np.float32)\n",
    "            vectors.append(vec)\n",
    "            # If we have our first 10 vectors, we can exit the loop\n",
    "            if len(vectors) == 10:\n",
    "                break\n",
    "    # The review does not have enough vectors, so we fill the rest with zeros\n",
    "    while len(vectors) < 10:\n",
    "        vectors.append(np.zeros((300,), dtype=np.float32))\n",
    "    # returns all the vectors, flattened to a single array of 3000 elements\n",
    "    return np.concatenate(vectors)\n",
    "\n",
    "# Replace the existing column for the review's input feature for both our w2v and googles w2v\n",
    "data['own_input_features'] = data['cleaned_reviews'].apply( \n",
    "    lambda text: create_first10_input_feature(text, w2v_own)\n",
    ")\n",
    "\n",
    "data['google_input_features'] = data['cleaned_reviews'].apply(\n",
    "    lambda text: create_first10_input_feature(text, w2v_google)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b.2 Training and Testing data split (Binary)\n",
    "Split the data into two distinct parts (80% training, 20% testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "binary_data = data[data['label'] <= 1] # Only select class 0 (positive) and class 1 (negative)\n",
    "own_input_features = binary_data['own_input_features']\n",
    "google_input_features = binary_data['google_input_features']\n",
    "binary_labels = binary_data['label']\n",
    "\n",
    "# Perform an 80-20 split for training and testing data on the binary data only\n",
    "X_train_own, X_test_own, y_train_own, y_test_own = train_test_split(\n",
    "    own_input_features,\n",
    "    binary_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Reshape from (num_samples,) to (num_samples, 3000)\n",
    "X_train_own = np.vstack(X_train_own)\n",
    "X_test_own = np.vstack(X_test_own)\n",
    "\n",
    "X_train_google, X_test_google, y_train_google, y_test_google = train_test_split(\n",
    "    google_input_features,\n",
    "    binary_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Reshape from (num_samples,) to (num_samples, 3000)\n",
    "X_train_google = np.vstack(X_train_google)\n",
    "X_test_google = np.vstack(X_test_google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b.3 First-10 Features with My Word2Vec (Binary Case)\n",
    "Use the first-10 input features from our trained Word2Vec into the binary MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Own W2V - First-10 MLP - Binary: accuracy is 0.779.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Binary MLP - First-10 Vectors - Own Trained Word2Vec\n",
    "\n",
    "# Prepare the data for PyTorch\n",
    "train_loader, test_loader = prepare_data(X_train_own, X_test_own, y_train_own, y_test_own, batch_size=32)\n",
    "\n",
    "# Create NN for binary classification\n",
    "model = MLP(num_input=300*10, num_classes=2, learning_rate=1e-5)\n",
    "\n",
    "# Train the NN\n",
    "model.train(max_epochs=50, data_loader=train_loader)\n",
    "\n",
    "# Evaluate NN\n",
    "model.report_accuracy('Own W2V - First-10 MLP - Binary', data_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b.4 First-10 Features with Google Word2Vec (Binary Case)\n",
    "Use the first-10 input features from Google's pre-trained Word2Vec into the binary MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google W2V - First-10 MLP - Binary: accuracy is 0.765.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Binary MLP - First-10 Vectors - Google News Trained Word2Vec\n",
    "\n",
    "# Prepare the data for PyTorch\n",
    "train_loader, test_loader = prepare_data(X_train_google, X_test_google, y_train_google, y_test_google, batch_size=32)\n",
    "\n",
    "# Create NN for binary classification\n",
    "model = MLP(num_input=300*10, num_classes=2, learning_rate=1e-5)\n",
    "\n",
    "# Train the NN\n",
    "model.train(max_epochs=50, data_loader=train_loader)\n",
    "\n",
    "# Evaluate NN\n",
    "model.report_accuracy('Google W2V - First-10 MLP - Binary', data_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b.5 Training and Testing data split (Ternary)\n",
    "Split the data into two distinct parts (80% training, 20% testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "own_input_features = data['own_input_features']\n",
    "google_input_features = data['google_input_features']\n",
    "ternary_labels = data['label']\n",
    "\n",
    "# Perform an 80-20 split for training and testing data on the complete dataset\n",
    "X_train_own, X_test_own, y_train_own, y_test_own = train_test_split(\n",
    "    own_input_features,\n",
    "    ternary_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Reshape from (num_samples,) to (num_samples, 3000)\n",
    "X_train_own = np.vstack(X_train_own)\n",
    "X_test_own = np.vstack(X_test_own)\n",
    "\n",
    "X_train_google, X_test_google, y_train_google, y_test_google = train_test_split(\n",
    "    google_input_features,\n",
    "    ternary_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Reshape from (num_samples,) to (num_samples, 3000)\n",
    "X_train_google = np.vstack(X_train_google)\n",
    "X_test_google = np.vstack(X_test_google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b.6 First-10 Features with My Word2Vec (Ternary Case)\n",
    "Use the first-10 input features from our trained Word2Vec into the ternary MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Own W2V - First-10 MLP - Ternary: accuracy is 0.622.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ternary MLP - First-10 Vectors - Own Trained Word2Vec\n",
    "\n",
    "# Prepare the data for PyTorch\n",
    "train_loader, test_loader = prepare_data(X_train_own, X_test_own, y_train_own, y_test_own, batch_size=32)\n",
    "\n",
    "# Create NN for ternary classification\n",
    "model = MLP(num_input=300*10, num_classes=3, learning_rate=1e-5)\n",
    "\n",
    "# Train the NN\n",
    "model.train(max_epochs=50, data_loader=train_loader)\n",
    "\n",
    "# Evaluate NN\n",
    "model.report_accuracy('Own W2V - First-10 MLP - Ternary', data_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b.7 First-10 Features with Google Word2Vec (Ternary Case)\n",
    "Use the first-10 input features from Google's pre-trained Word2Vec into the ternary MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google W2V - First-10 MLP - Ternary: accuracy is 0.611.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ternary MLP - First-10 Vectors - Google News Trained Word2Vec\n",
    "\n",
    "# Prepare the data for PyTorch\n",
    "train_loader, test_loader = prepare_data(X_train_google, X_test_google, y_train_google, y_test_google, batch_size=32)\n",
    "\n",
    "# Create NN for binary classification\n",
    "model = MLP(num_input=300*10, num_classes=3, learning_rate=1e-5)\n",
    "\n",
    "# Train the NN\n",
    "model.train(max_epochs=50, data_loader=train_loader)\n",
    "\n",
    "# Evaluate NN\n",
    "model.report_accuracy('Google W2V - First-10 MLP - Ternary', data_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. First-10 Features - Conclusion\n",
    "Overall, both models, our Word2Vec and Google News Word2Vec, performed similarly. For the binary case, their accuracy was 0.779 and 0.765 respectively. Likewise, for the ternary case it was 0.622 and 0.611 respectively. These values are similar to that of Q4a (using averaged vectors), but slightly lower.\n",
    "\n",
    "Similar to my justification in Q4a, the reason the ternary case has significantly lower accuracy is due to the way the data is imbalanced. Moreover, the accuracy of First-10 is lower than Averaged input features; this is because when we take only the first 10 vectors, we discard all the rest, thus incomplete data for the model to work with (i.e. by taking only the first 10, we lost part of the data)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
