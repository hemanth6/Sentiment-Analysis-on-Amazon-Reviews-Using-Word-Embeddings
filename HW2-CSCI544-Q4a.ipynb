{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure reproducibility\n",
    "Use a fixed seed such that all steps and results can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed handpicked to ensure all of the cleaning/pre-processing steps were visually shown\n",
    "SEED = 544\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feedforward NN\n",
    "In this section, we will train a Multilayer Perceptron for sentiment analysis classification for both the binary and ternary cases. Per the homework requirements, our network will consists of two hidden layers, each with 50 and 10 nodes, respectively. We will use cross entropy loss and ADAM for optimizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functionality\n",
    "Below are functions and classes that group up implementations for code reuse and understanding. It is used for Q4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Credit: From PyTorch's documentation\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Returns training and testing data loaders\n",
    "def prepare_data(X_train, X_test, y_train, y_test, batch_size):\n",
    "    train_data = torch.tensor(X_train, device=device)\n",
    "    train_label = torch.tensor(y_train.values, dtype=torch.long, device=device)\n",
    "    train_tensor = TensorDataset(train_data, train_label)\n",
    "    train_loader = DataLoader(dataset=train_tensor, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    test_data = torch.tensor(X_test, device=device)\n",
    "    test_label = torch.tensor(y_test.values, dtype=torch.long, device=device)\n",
    "    test_tensor = TensorDataset(test_data, test_label)\n",
    "    test_loader = DataLoader(dataset=test_tensor, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# A Multilayer Perceptron with 2 hidden layers that uses CrossEntropyLoss and Adam optimizer\n",
    "class MLP:\n",
    "    # Creates a model with Cross Entropy Loss and Adam optimizer\n",
    "    def __init__(self, num_input, num_classes, learning_rate):\n",
    "        self.model = MLP.create_model(num_input, 50, 10, num_classes)\n",
    "        self.model.apply(MLP.initialize_weights)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "    # Trains the model given the data_loader with max_epochs\n",
    "    def train(self, max_epochs, data_loader):\n",
    "        self.model.train()\n",
    "        for epoch in range(max_epochs):\n",
    "            for idx, (X, y) in enumerate(data_loader):\n",
    "                X = X.to(device=device)\n",
    "                y = y.to(device=device)\n",
    "                y_pred = self.model(X)\n",
    "                loss = self.criterion(y_pred, y)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "    # Returns the model's accuracy (0-1) given the data_loader\n",
    "    def evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in data_loader:\n",
    "                X = X.to(device=device)\n",
    "                y = y.to(device=device)\n",
    "                scores = self.model(X)\n",
    "                _, predictions = scores.max(1)\n",
    "                num_correct += (predictions == y).sum()\n",
    "                num_samples += predictions.size(0)\n",
    "        return float(num_correct) / float(num_samples)\n",
    "\n",
    "    # Reports the accuracy of the model\n",
    "    def report_accuracy(self, text, data_loader):\n",
    "        accuracy = self.evaluate(data_loader)\n",
    "        print(f'{text}: accuracy is {accuracy:.3f}.')\n",
    "        print()\n",
    "\n",
    "    # Creates a 2 hidden layer NN with the specified sizes, with ReLU activations\n",
    "    @staticmethod\n",
    "    def create_model(n_input, n_h1, n_h2, n_classes):\n",
    "        model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_input, n_h1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(n_h1, n_h2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(n_h2, n_classes)\n",
    "        )\n",
    "        model.to(device)\n",
    "        return model\n",
    "    \n",
    "    # Initializes linear layers to 0 with a bias of 1\n",
    "    @staticmethod\n",
    "    def initialize_weights(m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.zeros_(m.weight)\n",
    "            torch.nn.init.ones_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "Load the pandas dataset from Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from disk\n",
    "data = pd.read_pickle('dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load both Word2Vec models\n",
    "Load the w2v models from Q2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v_google = api.load('word2vec-google-news-300')\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "w2v_own = KeyedVectors.load('my_w2v.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Averaged Input Features in Multi-Layer Perceptron\n",
    "In this section, we shall build an MLP that takes in the averaged vectors as its input feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.1 Create the averaged input features for the reviews\n",
    "Input feature is the average Word2Vec vector for each review. Words with no encoding vectors are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the given review body text into the averaged Word2Vec vector using a given trained word2vec model\n",
    "def create_avg_input_feature(text, wv):\n",
    "    vec = np.zeros((300,), dtype=np.float32)\n",
    "    count = 0\n",
    "    # Will skip words that have no vectors\n",
    "    for word in str(text).split():\n",
    "        if word in wv:\n",
    "            vec += np.array(wv[word], dtype=np.float32)\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "# Create a new column for the review's input feature for both our w2v and googles w2v\n",
    "data['own_input_features'] = data['cleaned_reviews'].apply(\n",
    "    lambda text: create_avg_input_feature(text, w2v_own)\n",
    ")\n",
    "\n",
    "data['google_input_features'] = data['cleaned_reviews'].apply(\n",
    "    lambda text: create_avg_input_feature(text, w2v_google)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.2 Training and Testing data split (Binary)\n",
    "Split the data into two distinct parts (80% training, 20% testing) so that there is no overlap. This is done to ensure no data leakage nor bias influences the training and we can have a better view of the training process (if it overfitted for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "binary_data = data[data['label'] <= 1] # Only select class 0 (positive) and class 1 (negative)\n",
    "own_input_features = binary_data['own_input_features']\n",
    "google_input_features = binary_data['google_input_features']\n",
    "binary_labels = binary_data['label']\n",
    "\n",
    "# Perform an 80-20 split for training and testing data on the binary data only\n",
    "X_train_own, X_test_own, y_train_own, y_test_own = train_test_split(\n",
    "    own_input_features,\n",
    "    binary_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Reshape from (num_samples,) to (num_samples, 300)\n",
    "X_train_own = np.vstack(X_train_own)\n",
    "X_test_own = np.vstack(X_test_own)\n",
    "\n",
    "X_train_google, X_test_google, y_train_google, y_test_google = train_test_split(\n",
    "    google_input_features,\n",
    "    binary_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Reshape from (num_samples,) to (num_samples, 300)\n",
    "X_train_google = np.vstack(X_train_google)\n",
    "X_test_google = np.vstack(X_test_google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.3 Averaged Features with My Word2Vec (Binary Case)\n",
    "Use the averaged input features from our trained Word2Vec into the binary MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Own W2V - Averaged MLP - Binary: accuracy is 0.839.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Binary MLP - Averaged Vectors - Own Trained Word2Vec\n",
    "\n",
    "# Prepare the data for PyTorch\n",
    "train_loader, test_loader = prepare_data(X_train_own, X_test_own, y_train_own, y_test_own, batch_size=32)\n",
    "\n",
    "# Create NN for binary classification\n",
    "model = MLP(num_input=300, num_classes=2, learning_rate=1e-5)\n",
    "\n",
    "# Train the NN\n",
    "model.train(max_epochs=50, data_loader=train_loader)\n",
    "\n",
    "# Evaluate NN\n",
    "model.report_accuracy('Own W2V - Averaged MLP - Binary', data_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.4 Averaged Features with Google Word2Vec (Binary Case)\n",
    "Use the averaged input features from Google's pre-trained Word2Vec into the binary MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google W2V - Averaged MLP - Binary: accuracy is 0.818.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Binary MLP - Averaged Vectors - Google News Trained Word2Vec\n",
    "\n",
    "# Prepare the data for PyTorch\n",
    "train_loader, test_loader = prepare_data(X_train_google, X_test_google, y_train_google, y_test_google, batch_size=32)\n",
    "\n",
    "# Create NN for binary classification\n",
    "model = MLP(num_input=300, num_classes=2, learning_rate=1e-5)\n",
    "\n",
    "# Train the NN\n",
    "model.train(max_epochs=50, data_loader=train_loader)\n",
    "\n",
    "# Evaluate NN\n",
    "model.report_accuracy('Google W2V - Averaged MLP - Binary', data_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.5 Training and Testing data split (Ternary)\n",
    "Split the data into two distinct parts (80% training, 20% testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "own_input_features = [[col for col in row] for row in data['own_input_features']]\n",
    "google_input_features = [[col for col in row] for row in data['google_input_features']]\n",
    "ternary_labels = data['label']\n",
    "\n",
    "# Perform an 80-20 split for training and testing data on the complete dataset\n",
    "X_train_own, X_test_own, y_train_own, y_test_own = train_test_split(\n",
    "    own_input_features,\n",
    "    ternary_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "X_train_google, X_test_google, y_train_google, y_test_google = train_test_split(\n",
    "    google_input_features,\n",
    "    ternary_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.6 Averaged Features with My Word2Vec (Ternary Case)\n",
    "Use the averaged input features from our trained Word2Vec into the ternary MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Own W2V - Averaged MLP - Ternary: accuracy is 0.668.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ternary MLP - Averaged Vectors - Own Trained Word2Vec\n",
    "\n",
    "# Prepare the data for PyTorch\n",
    "train_loader, test_loader = prepare_data(X_train_own, X_test_own, y_train_own, y_test_own, batch_size=32)\n",
    "\n",
    "# Create NN for ternary classification\n",
    "model = MLP(num_input=300, num_classes=3, learning_rate=1e-5)\n",
    "\n",
    "# Train the NN\n",
    "model.train(max_epochs=50, data_loader=train_loader)\n",
    "\n",
    "# Evaluate NN\n",
    "model.report_accuracy('Own W2V - Averaged MLP - Ternary', data_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.7 Averaged Features with Google Word2Vec (Ternary Case)\n",
    "Use the averaged input features from Google's pre-trained Word2Vec into the ternary MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google W2V - Averaged MLP - Ternary: accuracy is 0.646.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ternary MLP - Averaged Vectors - Google News Trained Word2Vec\n",
    "\n",
    "# Prepare the data for PyTorch\n",
    "train_loader, test_loader = prepare_data(X_train_google, X_test_google, y_train_google, y_test_google, batch_size=32)\n",
    "\n",
    "# Create NN for binary classification\n",
    "model = MLP(num_input=300, num_classes=3, learning_rate=1e-5)\n",
    "\n",
    "# Train the NN\n",
    "model.train(max_epochs=50, data_loader=train_loader)\n",
    "\n",
    "# Evaluate NN\n",
    "model.report_accuracy('Google W2V - Averaged MLP - Ternary', data_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Averaged Features - Conclusion\n",
    "Overall, both models, our Word2Vec and Google News Word2Vec, performed similarly. For the binary case, their accuracy was 0.839 and 0.818 respectively. Likewise, for the ternary case it was 0.668 and 0.646 respectively.\n",
    "\n",
    "I believe the reason the ternary case has significantly lower accuracy is due to the way the data is imbalanced. The neutral class has half the data that the other classes have (50k vs 100k). Thus, this can skew the training and harm the model's learning.\n",
    "\n",
    "Moreover, our w2v model has better accuracy than Google's model is because of the nature of the model; our model was trained on dataset specifically for reviews whereas Google used generic news articles.\n",
    "\n",
    "Additionally, this accuracy is similar to that of earlier models in Q3 where they had an average accuracy of roughly 0.80 aswell. In order of better to worse it was SVM > Averaged MLP > Perceptron."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
