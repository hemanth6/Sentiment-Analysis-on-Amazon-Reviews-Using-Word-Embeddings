{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sanavesa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install nltk\n",
    "# !pip install bs4\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure reproducibility\n",
    "Use a fixed seed such that all steps and results can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 544\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "Load the data locally while skipping on lines that contains errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz\n",
    "# Load the data from disk\n",
    "url = 'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz'\n",
    "data = pd.read_table(url,sep = '\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings\n",
    "Strip the dataset to use only the two columns we're interested in, while dropping all rows that have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the 2 columns we need and remove rows that have missing values\n",
    "data = data[['review_body', 'star_rating']].copy()\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Balanced Dataset\n",
    "Only keep 250K reviews along with their ratings. That is 50K reviews for each rating score (1-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_samples = []\n",
    "for rating in range(1,6):\n",
    "    sample = data[ data['star_rating'] == rating].sample(50000)\n",
    "    rating_samples.append(sample)\n",
    "data = pd.concat(rating_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling Reviews\n",
    "Positive sentiment (class 1) is a rating of more than 3. Negative sentiment (class 2) is a rating less than 3. Neutral sentiment (class 3) have a rating of 3. Here, we create a new column for the review's sentiment label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the given rating (1-5) into its respective sentiment class\n",
    "def label_review(rating):\n",
    "    if rating > 3:\n",
    "        return 0\n",
    "    elif rating < 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# Create a new column for the review's sentiment label\n",
    "data['label'] = data['star_rating'].apply(label_review).astype('int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Clean and pre-process the data to improve performance, right before generating input features for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanavesa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:24: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sanavesa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Create a new column for the cleaned/processed reviews\n",
    "# It shall follow the same procedure from HW1.\n",
    "\n",
    "###### Convert to lower case\n",
    "data['cleaned_reviews'] = data['review_body'].str.lower()\n",
    "\n",
    "###### Remove HTML tags and URLs from a string\n",
    "def sanitize_review(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(str(text), 'html.parser').get_text()\n",
    "    # remove URLS\n",
    "    text = re.sub(r'http\\S+', '', str(text))\n",
    "    return text\n",
    "data['cleaned_reviews'] = data['cleaned_reviews'].apply(sanitize_review)\n",
    "\n",
    "###### Use a library to expand the contractions as it includes a plethora of pre-defined contractions\n",
    "# !pip install contractions\n",
    "import contractions\n",
    "def fix_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "data['cleaned_reviews'] = data['cleaned_reviews'].apply(fix_contractions)\n",
    "\n",
    "###### Remove all characters but keep english characters and space\n",
    "data['cleaned_reviews'] = data['cleaned_reviews'].str.replace('[^a-zA-Z\\s]', ' ')\n",
    "\n",
    "###### Remove all unnecessary spaces\n",
    "def remove_extra_spaces(text):\n",
    "    return ' '.join(str(text).split())\n",
    "data['cleaned_reviews'] = data['cleaned_reviews'].apply(remove_extra_spaces)\n",
    "\n",
    "###### Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "# Split each review into a list of words, then eliminate those words that are in the stopwords set as provided by nltk\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join([word for word in str(text).split() if word not in (stop)])\n",
    "data['cleaned_reviews'] = data['cleaned_reviews'].apply(remove_stop_words)\n",
    "\n",
    "###### Lemmatize reviews\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in tokenizer.tokenize(text)])\n",
    "# Use NLTK lemmatizer with verb as its part of speech to reduce inflections\n",
    "data['cleaned_reviews'] = data['cleaned_reviews'].apply(lemmatize)\n",
    "\n",
    "###### Drop rows that have no text after cleaning and preprocessing\n",
    "data = data[data['cleaned_reviews'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Google News Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Google News Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v_google = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check semantic similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king-man+woman =~  [('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951)]\n",
      "car =~  [('vehicle', 0.7821096181869507), ('cars', 0.7423831224441528), ('SUV', 0.7160962224006653)]\n",
      "excellent =~  [('terrific', 0.7409726977348328), ('superb', 0.7062715888023376), ('exceptional', 0.681470513343811)]\n",
      "beautiful =~  [('gorgeous', 0.8353005051612854), ('lovely', 0.8106936812400818), ('stunningly_beautiful', 0.7329413294792175)]\n",
      "angry =~  [('irate', 0.8138925433158875), ('enraged', 0.7705066800117493), ('indignant', 0.7013434171676636)]\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "\n",
    "# king - man + woman =~ queen\n",
    "print('king-man+woman =~ ', w2v_google.most_similar(positive=['woman', 'king'], negative=['man'], topn=3))\n",
    "\n",
    "# car =~ vehicle\n",
    "print('car =~ ', w2v_google.most_similar(positive=['car'], topn=3))\n",
    "\n",
    "# excellent =~ terrific\n",
    "print('excellent =~ ', w2v_google.most_similar(positive=['excellent'], topn=3))\n",
    "\n",
    "# beautiful =~ gorgeous\n",
    "print('beautiful =~ ', w2v_google.most_similar(positive=['beautiful'], topn=3))\n",
    "\n",
    "# angry =~ irate\n",
    "print('angry =~ ', w2v_google.most_similar(positive=['angry'], topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. My Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Word2Vec on our dataset\n",
    "Use gensim's Word2Vec implementation to train our dataset and learn vector encodings from the unprocessed reviews. Then save the model to disk to use it in other parts of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "# Stream the reviews one by one\n",
    "class MyCorpus:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for review in self.data['review_body']:\n",
    "            yield utils.simple_preprocess(review)\n",
    "\n",
    "# Train a Word2Vec model using the UNPROCESSED reviews with the specified parameters\n",
    "from gensim.models import Word2Vec\n",
    "sentences = MyCorpus(data)\n",
    "# Using a single worker with a fixed seed to ensure the results are identical on every machine\n",
    "w2v_own = Word2Vec(sentences=sentences, min_count=10, vector_size=300, window=11, epochs=10, seed=SEED, workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check semantic similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king-man+woman =~  [('queen', 0.4555205702781677), ('arthur', 0.38737979531288147), ('kaiser', 0.37041571736335754)]\n",
      "car =~  [('backpack', 0.7148886919021606), ('purse', 0.7141773104667664), ('vehicle', 0.6302756071090698)]\n",
      "excellent =~  [('outstanding', 0.7944633960723877), ('exceptional', 0.7194543480873108), ('incredible', 0.6430865526199341)]\n",
      "beautiful =~  [('gorgeous', 0.7997910976409912), ('lovely', 0.7912963032722473), ('stunning', 0.7248261570930481)]\n",
      "angry =~  [('upset', 0.7107461094856262), ('annoyed', 0.7074971795082092), ('irritated', 0.6425274014472961)]\n"
     ]
    }
   ],
   "source": [
    "# king - man + woman =~ queen\n",
    "print('king-man+woman =~ ', w2v_own.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=3))\n",
    "\n",
    "# car =~ vehicle\n",
    "print('car =~ ', w2v_own.wv.most_similar(positive=['car'], topn=3))\n",
    "\n",
    "# excellent =~ terrific\n",
    "print('excellent =~ ', w2v_own.wv.most_similar(positive=['excellent'], topn=3))\n",
    "\n",
    "# beautiful =~ gorgeous\n",
    "print('beautiful =~ ', w2v_own.wv.most_similar(positive=['beautiful'], topn=3))\n",
    "\n",
    "# angry =~ irate\n",
    "print('angry =~ ', w2v_own.wv.most_similar(positive=['angry'], topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save My Word2Vec to disk\n",
    "Save the keyed vectors of the trained word2vec to be used for the other parts of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_own.wv.save('my_w2v.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Comparison between Google W2V and My W2V\n",
    "Overall, the vectors generated by our model and the pretrained model are similar.\n",
    "\n",
    "In both models, the 'excellent', 'beautiful', and 'angry' cases resulted in appropriately similar results with great semantic similarities (i.e., beautiful ~= gorgeous with 0.80 similarity). Likewise, the 'king-man+woman' results in 'queen' in both models; however, the Google News model has a higher semantic similarity than our model, 0.711 vs 0.439 respectively. This is partly due to the differing nature of the dataset: our dataset is on reviews whereas the other used Google News written professionally.\n",
    "\n",
    "Finally, in the Google News model, the 'car' word was similar to 'vehicle' with 0.78 semantic similarity while being the most similar, whereas in our model the word 'car' and 'vehicle' had a 0.65 semantic similarity but it ranked 3rd after 'backpack' and 'purse'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Save dataset to disk\n",
    "Save the trimmed, processed dataset to disk to be used in the other parts of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Remove unnecessary columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanavesa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  \n",
      "c:\\users\\sanavesa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Remove rating and review_body columns as they are not needed to save space\n",
    "data.drop('review_body', 1, inplace=True)\n",
    "data.drop('star_rating', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Save processed dataset to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('dataset.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
